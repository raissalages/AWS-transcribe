 are a few ways that we can send data to kinesis data streams and risk and other things could see I can consume data from data streams. So for producers, we have the Amazon kinesis agent. This is a standalone Java app that will monitor files based on a pattern and send that data over to kinesis. Whenever it sees any kind of changes, you have to understand how Java apps to work to deploy this. So you don't necessarily have to write Java code, but you definitely know how to run Java apps. So this is where Java becomes useful. You have the AWS SDK where you can use put put records. But this is not really intended to be used for production. Because if you're working with kinesis, you're working with a vast amount of data. And this is just not going to cut it in terms of at scale. Now, if you're learning how to use kinesis is a great way to learn it. But for real applications, you're going to be doing something else. There's a there's many other services will that will directly send data over to data streams, and they might require configuration to do so. I didn't. At least when I did the labs, I didn't test for this. But it says there's tons of services that can do this, I think it's gonna be the same list when we look at data firehose that can send send data over there. But the idea is that it has its own event bus and can send data over there or some can be configured to do so. Then you have the Amazon kinesis producer library, the KPL. This is a Java only library that allows you to scale at scale publish data to the data stream. This is the preferred method for implementing a producer, you have to know how to use Java to do this. You have to write Java code. So this is a use case where Java becomes very important. For consumers. You can send data to third parties. So they have integrations for this. So you can send it to Pachi fink Kafka Connect Adobe experience, Oracle Golden Gate, dead beds, Deb's zeb zn, I can't pronounce that fluent D. So Apache fink is definitely one that you might want to utilize. And also consider that AWS has a managed version of Pachi fink, which is the the data streams analytics. So that could be a great place to send your data or integrate, integrate with, then you have kinesis data firehose. And this is a great option when you want to send to a lot of services. We'll cover what those services are in the data firehose section. But the idea is that you can send it to the firehose and they can send it to three fire redshift and a lot of other stuff. You can use the SDK to read records using the get records, not for scale. So great for learning but not for scale. Then you have the KCL library. Why they didn't call the kinesis consumer library? I don't know. But it's called the kinesis client library. AWS, why don't you name these things better. But anyway, it's the client library. And it is a Java library to write your own custom consumers. But what's interesting is that it has this multi line daemon. So it allows you to actually leverage the Java underneath in another language. So they have this for for Python and for Ruby, maybe for other other languages. Because the likelihood of people needing to write consumers is like you can probably get away with not writing a producer if you if you're using direct integration, or use the kinesis agent, you don't really have to learn learn how to write Java. But for consumers, you absolutely have to write code. And so they gave you more flexibility here.