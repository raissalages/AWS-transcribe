 just to wrap everything up, we have a really good redshift cheat sheet here, definitely recommend you print this out for your exam. And we're gonna go through everything again. So data can be loaded from s3, Mr DynamoDB or multiple data sources on remote hosts. Redshift is columnar columnar store database, which can give you SQL like queries and is and is an old app. Redshift can handle petabytes worth of data Redshift is for data warehousing. Redshift's most common use cases business intelligence. Redshift can only run in one AZ. So it's a sink, it's single AZ it's not multi AZ Redshift can run via a single node or multi node for clusters. A single node is 160 gigabytes in size. A multi node is comprised of the leader node and multiple compute nodes. You are billed per hour for each node excluding the leader node in multi node, you're not billed for the leader node just repeating that again there, you can have up to 128 compute nodes. And again, I said earlier that the maximum by default was 32. But they're not going to ask you what the the default is. Redshift has two kinds of node types, dense compute and dense storage. And it should be pretty obvious when you should use one or the other. Redshift attempts to back back up your data three times the original on the compute node and on s3. Similar data is stored on a disk sequentially for faster reads. regif data database can be encrypted via kms or cloud HSM. backup retention is default to one day and can be increased to a maximum of 35 days. Redshift can asynchronously backup to your snap to your backup via snapshot to another region delivered via s3. And Redshift uses massively parallel processing to distribute queries and data across all loads. And in the case of an empty empty table when importing, Redshift will sample the data to create a schema. So there you go. That's Redshift in a nutshell. And that should help you for the exams.