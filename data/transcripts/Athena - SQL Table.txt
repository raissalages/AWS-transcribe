 Alright, we're taking a look at Athena SQL tables. And these can be created in two ways. The first is using SQL create table statement. This is where you're just going to write an SQL statement within the management console and Athena. The other way is using the AWS glue wizard, some services will create the tables automatically for you. So you might be creating a table and you don't exactly realize it. tables can be created automatically using the AWS glue crawler, which will crawl the data to produce a table schema. Athena tables are AWS glue data catalog tables. And so they will exist in both services when creating an Athena table. At one point, glue data catalog did not exist. So I don't exactly know how it worked before, but it worked a bit differently. But now this is the way it works. So that's totally fine. So when you query from when you do a query from for your table, you're here, we're going to use AWS data catalog. So that would be our data source. Often there's always an AWS data catalog table. So the idea here is we have our our data source, our database, and our table name, okay. tables are likely to be created in the default database default. And I noticed that there is a default like this is my opinion, because I noticed that there is a default one. And so I think that some programs or some services, like if you press a button, it will make a default, a default database there. But I sometimes it's not there by default. So I'm thinking AWS makes that at some point for you. Okay, using SQL, you can specify a few things, how to parse each row of the data, possibly using regex. And we will talk about that in separate slide, specific location of that data set should have a T in the space here, whatever, sorry about that, we'll fix that post here. And so here is an example of an SQL statement. So let's just take a look at creating the table. This is actually this SQL create table statement up here. So we say create table. And if it doesn't, if it does not exist, created, if it doesn't exist, I'm calling the table cloud front logs. Here you see we have our data type, or sorry, the name of it, and then our data type knows that it's in cap capitals. A lot of times in SQL languages, these things are not case case sensitive, the name of your columns can be, but the names of things like your data types, or the from statement or other stuff is going to vary. Then down below here, notice that we have this row format, sir, de, which is serialization deserialization, this is going to determine how it parses the data in the S in the s3 files. And so in this case, we're using hives, hives, serial deserialization, and it's using regular expressions to parse the s3 files, the file is located in s3. If there was any other source, I've never seen anything else other than s3. But there could be but anyway, that is that but we'll talk about sir, de or serialization deserialization more coming up here shortly. But there you go.